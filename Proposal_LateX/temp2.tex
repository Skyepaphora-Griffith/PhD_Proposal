\documentclass{article}

%%%%%%%% SKYEPAPHORA %%%%%%%%
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{xcolor}
\usepackage{cancel}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{setspace}
\usepackage{listings}
\usepackage{multicol}
\usepackage{textcomp}
\usepackage{centernot}
\usepackage{bbm}
\usepackage{mathrsfs}
\usepackage{hyperref}
\hypersetup{
    colorlinks=false,
    linkcolor=blue
    }

\setlength{\fboxsep}{8pt}
% \setlength{\parindent}{0pt}

\newcommand{\define}{\ensuremath \stackrel{\text{def}}{=}}
\newcommand{\Var}{\ensuremath \text{Var}}
\newcommand{\Cov}{\ensuremath \text{Cov}}
\newcommand{\E}{\ensuremath \text{E}}
\newcommand{\myrule}{\rule{\linewidth}{0.5pt}}

\definecolor{gold}{HTML}{A05000}
\definecolor{redd}{HTML}{C00000}
\definecolor{bluu}{HTML}{0000C0}
\definecolor{gren}{HTML}{00B000}

\definecolor{salt}{HTML}{FBFAF9}
\definecolor{pepper}{HTML}{251F18}

\begin{document}
\pagecolor{salt}\color{pepper}
{\noindent\Huge\textbf{Background \\[5pt] {\LARGE and} Literature Review}}

%%% SPECTRUM %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Spectrum Estimation \\ {\normalsize and the} Multitaper Method}

\subsection{Spectral Representations}
    \fbox{
    \begin{minipage}{0.93\linewidth}
    \subsubsection*{Brockwell \& Davis}
    \begin{itemize}
        \item knowledge from Spectrum course (undergrad)
    \end{itemize}
    \subsubsection*{Percy and Waldy 1996}
    \subsubsection*{Percy and Waldy - the one I read?}
    \begin{itemize}
        \item Knowledge from Spectrum course (master's)
    \end{itemize}
    \end{minipage}
    }\\[8pt]

Intro
% ---------
\subsubsection{Autocovariance}
Consider a complex time series $X(t),\, t\in \mathbb Z$. The covariance kernel in equation \ref{eq:1} describes the series' covariance with itself across time: the \textit{auto}-covariance structure. \footnote{Note: we assume $X(t)$ is zero-mean without loss of generality.} The kernel's other parameter, $h$, is an integer representing the \textit{lag} between 2 time points.
\begin{flalign}
    &\text{General Case}&
        \gamma_t(h) &\define\Cov \big[X(t), X(t+h)\big] \qquad t \in \mathbb Z
        \label{eq:1}& \\[7pt]
    &\text{Stationary Case}&    
        \gamma  (h) &\define\Cov \big[X(s), X(s+h)\big] \quad\, \forall s \in\mathbb Z
        \label{eq:2}&
\end{flalign}\\[-8pt]
Now assume that $X$ is \textit{stationary}, meaning that its autocovariance structure does not change over time. The absolute location $t$ becomes irrelevant, and the covariance kernel may be written as an autocovariance function (ACVF) of lag, as in equation \ref{eq:2}.  \\

\textit{Continue with properties of ACVF}\\
Moreover, the autocovariance function of a stationary process is necessarily Hermitian (or, an \textit{even} function, in the real-valued case) and non-negative definite. [Source: B\&D chapter 1]\\ blablabla
% ---------

% ---------
\subsubsection{Spectral Representation of a Discrete-time Stationary Process}
{\color{gren} \textbf{\large Percy and Waldy, 93}

Suppose we have a discrete, finite, real valued time series $X(t)$, which is a sum of $J$ sinusoids with distinct frequencies:
\begin{equation}
    X(t) = \sum_{j=1}^{J} A_j\cos(2\pi f_jt + \phi_j)    \label{eq:3}
\end{equation}
Assuming $\Delta t = 1$, the above frequencies are restricted to the open interval (0,1/2). The only stochastic components of equation (\ref{eq:3}) are the phases $\{\phi\}_j$, who are independently and uniformly distributed on $[-\pi,\pi]$. The indices themselves are positive, thus we can define the following complex exponential functions:
\begin{flalign*}        
        C_j     &= (A_j e^{i\phi_j})/2      \\
        C_{-j}  &= (A_j e^{-i\phi_j})/2.     \\
    \intertext{Moreover, we deal with indices $j\leq 0$ by setting $C_0 = 0$, and,}
        f_0 \define 0; &\qquad f_{-j} \define -f_j   \\
        A_0 \define 0; &\qquad A_{-j} \define A_j.   
\end{flalign*}
Equation (\ref{eq:3}) can now be expressed as the sum
\begin{flalign}
        X(t) &= \frac{A_j}{2}\big(e^{i\phi_j}e^{i2\pi f_jt}
                                + e^{-i\phi_j}e^{-i2\pi f_jt}\big)  \notag\\
             &= \sum_{j=-J}^{J} C_je^{i 2\pi f_jt}.   \label{eq:4}
\end{flalign}  
The random variables $\{C\}_j$ are independent, uncorrelated, and zero-mean, with variances $\Var[C_j] = A^2_j/4$. Using the fact that the $\{f\}_j$ are deterministic, this gives us the variance of $X(t)$ itself:
\begin{flalign}
        \Var[X_t] &= \sum_{j=-J}^{J} A^2_j/4.
    \intertext{\textbf{Orthogonal Increments Processes}}
    \intertext{We now define a stochastic process of frequency, $Z(f)$, and with this, we consider how $Z$ behaves in tiny increments along a continuous spectrum of frequencies.}
        Z(f)  &= \\
        dZ(f) &=.
    \intertext{Above, we extend the frequency range from $(0,1/2)$ to $[0,1/2]$ by setting $Z(0)\define 0$, and $f_{J+1} \define 1/2$. At the discrete frequencies $f_j$, which originally defined $X(t)$, the increments can be written in terms of the exponentials we constructed earlier:}
        dZ(f_l) &= 
\end{flalign}
and from this we know that the increments $dZ(f)$ are zero-mean. They also happen to be uncorrelated under additional constraints: the \textit{intervals} $[f_1,f_1 + df_1]$ and $[f_2, f_2 + df_2]$ mustn't overlap, and must both be contained within the greater frequency interval [-1/2,1/2]. The (complex) covariance $\Cov[dZ(f_1),dZ(f_2)] = E[dZ^*(f_1)dZ(f_2)]$ is an inner product equal to zero for any $f_1, f_2$ satisfying these constraints, meaning these increments are effectively orthogonal. $Z(f)$ is, then, a process of orthogonal increments: an \textit{orthogonal increment process.}

With all that defined, we can contemplate how these orthogonal increments may be used to represent the original time process $X(t)$. Consider the following:
\begin{enumerate}
    \item As a function of frequency, $e^{i2\pi ft}$ is continuous on $[-1/2,1/2]$.
    \item The function $Z(f)$ is a step function on $[-1/2,1/2]$, with a total of $J$ steps. Each step corresponds to a finite change in value: $C_j$.
\end{enumerate}
Then by the definition of the Riemann-Stieltjes integral,
\begin{equation}
    X(t) = \sum_{j=1}^J e^{i2\pi f_jt} C_j
         = \int_{-1/2}^{1/2} e^{i2\pi ft} dZ(f). \label{eq:9}
\end{equation}
Equation (\ref{eq:9}) is called the \textit{Spectral Representation} of $X(t)$.\\

So, what advantage does this representation give us?
Suppose we want to find an expression for $\Cov[X(t), X(t+\tau)]$ (in other words, the autocovariance at $\gamma_t(\tau)$), using the spectral representations of these objects. Since these representations are (in theory) complex valued, the first term of the product within the expectation will need to be conjugated. 
\begin{flalign*}
    \Cov[X(t_1),X(t_2)] 
    &= E[X^*(t_1), X(t_2)] \\
    &= E\left [ \int_{-1/2}^{1/2} e^{i2\pi ft_1} dZ^*(f)
                \int_{-1/2}^{1/2} e^{i2\pi f't_2} dZ(f')
        \right]\\
    &=  \int_{-1/2}^{1/2}\int_{-1/2}^{1/2} e^{i2\pi ft_1}e^{i2\pi f't_2}
        E\big[dZ^*(f)dZ(f')\big].
\intertext{Using the orthogonality property of $dZ$, this integral becomes zero whenever $f\neq f'$. Moreover, if $X$ is \textit{stationary,}}
    \Cov[X(t),X(t+\tau)] 
    &= E[X^*(t), X(t+\tau)] \\
    &=  \int_{-1/2}^{1/2}\int_{-1/2}^{1/2} e^{i2\pi ft}e^{i2\pi f'(t+\tau)}
        E\big[dZ^*(f)dZ(f')\big].\\
    &=  \int_{-1/2}^{1/2}\int_{-1/2}^{1/2} e^{i2\pi(f'-f)t}e^{i2\pi f\tau}
        E\big[dZ^*(f)dZ(f')\big].\\
\end{flalign*}

}


``There exists an orthogonal increment process,
$\{Z(f)\},$ defined on the interval $[-1/2, 1/2],$ such that" 
\begin{equation}
    X(t) = \int_{-1/2}^{1/2} e^{2\pi ft}dZ(f)
\end{equation}
Where $Z$ is stochastic with increments such that: mean zero, orthogonal (autocoherence=0) and average mod squared = to increment of integrated spectrum. [PercyWaldy Spectral Univariate, p. 135. Also Priestly Chapter 4.11] \\

any stationary process can be represented as the sum of sines/cosines at varying frequencies with random amplitudes/phases. [Priestly 81] \\

Spectral density function introduced in eq. 111b [PercyWaldy] \\

If you want more, [B\&D 4.1 to 4.3 inclusive]

Fourier Transform pair between ACVF and Spectrum [any of the above + dAVE]
% ---------


\subsection{Multitaper}
    \fbox{
    \begin{minipage}{0.93\linewidth}
    \subsubsection{Slepian 1978 - Bell labs}
    \subsubsection{Daveâ€™s 1982 paper}
        \begin{itemize}
            \item Intro to Multitaper
            \item Derives Slepians as optimal
        \end{itemize}
    \subsubsection{My thesis if I want to describe eigencoefs?}
    \end{minipage}
    }\\[8pt]


%%% EPS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evolutionary Spectral Theory}
% ---------------------------------
\subsection{Approaches to Non-stationarity}

% ---------
\subsubsection{Thomson 93 - Nonstationary fluctuations in stationary time-series}
\begin{itemize}
    \item Detect stationarity in small sample time series
    \item  Methodology: decompose e.coef Cov matrix and power function into trace-orthogonal basis matrices
\end{itemize}

% ---------
\subsubsection{K. Rahim - Applications of Multitaper Spectral Analysis to Nonstationary Data}
\begin{itemize}
    \item Data: nonstationary grape harvest
    \item Estimate frequency variation: where and how much? +Partitioning series into stationary components
    \item Graphical test for spectral changes; simulation; application
    \item Goodness-of-fit test for AR estimators to estimate spectral changes over time
    \item Level of change estimator; Multitaper; new R package
\end{itemize}

% ---------
{\color{gold}
\subsubsection{Priestly 81}
\begin{itemize}
    \item Intuition: frequency structure of non-stationary series
\end{itemize}
\textbf{Section Introduction: \text{Motivation and Desirable Spectral Properties}}
\begin{enumerate}
    \item We want interpretable, easily estimable spectra for non-stationary series. 
    \item These should reveal stochastic structure, and support linear prediction/filtering.
    \item Evolutionary Spectra reveal frequency structure near some particular time point.
    \item Some non-stationary series can be broken into multiple stationary sub-series 
    \item These sub-series may have distinct autocovariance functions
\end{enumerate}

\textbf{Example: Short Time Fourier Transforms}\\[8pt]
Obtain average spectrum in a neighbourhood about time instant $t$.
\[x_t = x_t^{(i)} \qquad\qquad t_i < t \leq t_{i+1}\]

Each interval has a corresponding spectrum.\\
$\star$ Interpreted similarly to classical (as for stationary series) spectra \\
$\star$ Describes power-frequency distribution *locally* about an *instant* in time\\

\textbf{An Alternative notion of Frequency}\\[8pt]
Stationary $x_t$ have spectral representation $x_t = \int_{-\infty}^\infty e^{it\omega}dZ(\omega) \qquad (11.1.3)$\\

By (11.1.3), any stationary process can be represented as the sum of 
\textit{sines/cosines at varying frequencies} with \textit{random amplitudes/phases.} 
This lets us discuss the power (variance) contribution of a component distinguished by frequency $\omega$. \\

Suppose $x_t$ is a continuous, non-stationary process with ACVF
$\gamma(s,t)$, and consider a function $\rho(\omega,t)$ such that 
\[E\Big[x^2_t\Big] = \int_{\mathbb{R}} \rho(\omega,t) d\omega \qquad (11.1.1) \]
The issue is: $\rho$ is not necessarily a physically interpretable time dependent function of frequency.\\

\textbf{Example: Naive Approach to Time Dependent Spectra}
\begin{flalign*}
    &{\footnotesize\text{"Local" ACVF }} 
    &\gamma_\tau(t) &\stackrel{def}{=} \gamma\left(t-\frac{\tau}{2},\, t+\frac{\tau}{2}\right)
    & \;
    \\&&&&\\
    &{\footnotesize\text{Naive time dependent SDF}}
    &\psi_t(\omega) &= \frac{1}{2\pi}\int_{-\infty}^\infty \gamma\left(t-\frac{\tau}{2},\, t-\frac{\tau}{2}\right) e^{-i\omega\tau}d\tau 
    & \quad(11.1.2 a)    
    \\&&&&\\
    &{\color{redd}\footnotesize\begin{aligned}
        &\textbf{Azadeh's Notation}\\
        &\text{(Mean of Wigner-Ville)}
    \end{aligned}}
    & \color{redd} W_X(t,f) 
    & \color{redd} = \int_{\mathbb{R}} 
    E\left[
    X\Big(t + \frac{\tau}{2}\Big)
    \overline{X\Big(t - \frac{\tau}{2}}\Big)
    \right]
    e^{-i2\pi f\tau} d\tau
    & \color{redd} \forall t,f\in \mathbb{R}
    \\&&&&\\
    &{\footnotesize\text{FT pair at } t \text{ (} \tau = 0)} 
    &\gamma(t,t) &= \int_{-\infty}^{\infty} \psi_t(\omega)d\omega
    & \quad(11.1.2 b)
\end{flalign*}

\begin{itemize}
    \item $\psi_t(\omega)$ may take \textit{negative values.} So there isn't a good physical interpretation
    \item $\psi_t(\omega)$ not obviously interpretable wrt structure of $x_t$
    \item We need to define frequency without using sinusoids.\\
\end{itemize}

\textbf{Example: "Frequency" in the context of evolving spectra}

Suppose $x_t$ is a (deterministic) dampened sine:
\[x_t = Ae^{-t^2/\alpha^2}\cos (\omega_0 t + \phi) \qquad (11.1.4)\]
\begin{itemize}
    \item The FT of $x_t$ is 2 Gaussian Functions centred at $\omega_0, -\omega_0$, respectively. The width of the Gaussian functions is inversely proportional to $\alpha$.
    \item $x_t$ as a sum of sinusoids must include all frequency components
    \item $x_t$ can be represented by only the two frequencies $\omega_0$ and $-\omega_0$. Each component then has time varying amplitude $Ae^{-t^2/\alpha^2}$. If the time interval of observation is small relative to $\alpha$, $x_t$ will resemble (eq. 11.1.4)\\
\end{itemize}

\textbf{Oscillatory processes (See next section for more details)}\\
\[X(t) = \int_{\mathbb{R}} M(t,f)\, e^{i2\pi ft}\, dZ(f) \qquad \forall t\in\mathbb{R}\]
Where $M$ is a slowly varying (FT maxed at 0) modulating function. \\

\textbf{Non-oscillatory processes: } \\
Suppose $y_t$ behaves locally similar to a sine wave with fixed frequency and smooth amplitude modulation.
\begin{itemize}
    \item Frequency domain of \textit{oscillatory process:} FT is concentrated about $\omega_0$
    \item What if $y_t = \ln(\omega t)$? Then frequency has no meaning.
    \item Consider instead: the Fourier transform's absolute maximum. (Say, at point $\omega_1$) \\
\end{itemize}

\textbf{Summary}
\begin{enumerate}
    \item Autocovariance can be taken for balls surrounding a time $t$ of interest, and the Fourier transform of this ACVF has a pair converging to the variance of $x_t$ as the radius shrinks. However, this expression can take negative values and is often therefore not meaningful.
    \item Time series can be represented by 
            \begin{enumerate}
                \item infinitely many frequency components, with constant amplitudes OR
                \item frequency components with time-varying amplitudes.
            \end{enumerate}
    \item If $x_t$ does not oscillate, we define its "frequency" to be the absolute maximum of its freq. domain representation. We interpret this as the freq for a local sinusoidal representation whose amplitude is being modulated.
\end{enumerate}}




% ---------------------------------
\subsection{Evolutionary Power spectra}

% ---------
\subsubsection{Priestly 81}
\begin{itemize}
    \item Intro to Time Dependent Spectra
    \item Oscillatory functions
    \item Wold Cramer, Evolutionary power spectra
\end{itemize}

{\color{redd} \textbf{Oscillatory Functions/Processes:}  (p. 20 of A. pdf)\\
Let $X(t)$ be discrete, complex, zero-mean, finite variance. Suppose we can write the covariance function of $X$ as:
\begin{flalign*}
    R_X(t,s)    &= \int_{-1/2}^{1/2} \phi_t(f)\phi_s^*(f) S_X(f)df \qquad (3.1) \\[5pt]
    \phi_\cdot  &: [-1/2, 1/2] \to \mathbb{C} \\
    S_X         &: [-1/2, 1/2] \to \mathbb{R}
\end{flalign*}
Fix $f_0\in [-1/2, 1/2]$. Suppose $\phi_t(f_0)$ has an FT which maxes out at 
the frequency $\theta(f_0) \in [-1/2, 1/2]$.  Then we can write 
\[\phi_t(f) = M_t(f)e^{i2\pi\theta(f)t} \qquad\qquad (3.4)\]
Where $M_t(f_0)$ has an FT which maxes out at zero frequency. \textit{("Slowly varying").} 
Then $\phi_t$ is an osc. func., and $X$ is an osc. proc.\\

\textbf{Uniformly Modulated Processes:}  (p. 30 of A. pdf)\\
Let $X(t)$ be an osc. proc. and $\{C(t)\}_{t\in\mathbb Z}$ be non-negative, "slowly varying".
Let $Y(t)$ be zero-mean, stationary.
Then X is a unif. mod. proc. with evolutionary spectrum $S^E_X(t,f) = C^2(t)S_Y(f)$.\\

\textbf{Detecting UMPs}
A striking property of the TFS matrix $S_X(t,f)$ is that it has rank 1, since it is the matrix product of the $N\times 1$ vector $c^2(t)$, and the $1\times N_f$ vector $S_Y(f)$. This rank property on its own is not necessarily distinguished from the case of $X(t)$ being stationary, as the corresponding $S_X(t,f)$ would have $N$ identical rows (or equivalently, columns) and thus have rank 1, as well. 

The log-TFS matrix (LTFS) offers further insight in that it can be expressed as the sum of two matrices, each of which obviously having rank 1:
\begin{equation}
    \log \Big[S_X(t,f)\Big] = 2
    \begin{bmatrix}
        \log \big[c(1)\big]    & \dots & \log \big[c(N_f)\big] \\
        \vdots  & \dots & \vdots \\
        \log \big[c(1)\big]    & \dots & \log \big[c(N_f)\big] \\
    \end{bmatrix}
    +
    \begin{bmatrix}
        \log \big[S_Y(f_1)\big]    & \dots & \log \big[S_Y(f_{N_f})\big] \\
        \vdots  & \dots & \vdots \\
        \log \big[S_Y(f_1)\big]    & \dots & \log \big[S_Y(f_{N_f})\big] \\
    \end{bmatrix},
\end{equation}
This implies $\log\big[S_X(t,f)\big]$ must have rank $\leq 2$, if we assume $X$ is uniformly modulated. $X(t)$ being \textit{stationary}, however, would require $c(t)$ remain constant with respect to time, rendering each entry in the first matrix mutually identical, and forcing $\log\big[S_X(t,f)\big]$ to have rank no greater than 1. This distinction offers a method of detecting UMPs amongst the more restrictive class of stationary processes.

In practice, the stochastic foundations of these objects will contribute to their apparent ranks, and as such, we desire an empirical rank test of some kind. Current methods [??] propose a singular value decomposition of the TFS matrix: the number of non-negligible singular values will correspond to the rank of $S_X$, thereby indicating whether $X(t)$ may be a UMP. If the TFS is found to be of rank 1, performing a similar decomposition on the LTFS will further diagnose $X(t)$ as either stationary or uniformly modulated.\\

\textbf{Wold-Cramer Evolutionary Spectra:}  (p. 30 of A. pdf)\\
Let $X(t)$ be discrete, real, zero-mean, NON-stationary. Let $\epsilon(t)$ be white noise (0,1).
Then $X$ is the output of a causal $(n\leq t)$ time-varying linear filter.

\begin{flalign*}
        X(t) &= \sum_{n=-\infty}^t h(t,n)\epsilon(n)            &&\text{Linear Filter}&
    \\[5pt]
    \epsilon(t) &= \int_{-1/2}^{1/2}e^{i2\pi ft}dZ_\epsilon (f) &&\text{Spec rep }(\epsilon \text{ stationary})&
    \\[5pt]
    H_t(f) &= \sum_{n=-\infty}^t h(t,n) e^{-i2\pi f(t-n)}       &&\text{Generalized TF}&
    \\[8pt]\hline\\
    \implies X(t) &= \int_{-1/2}^{1/2} H_t(f) e^{i2\pi ft}dZ_\epsilon (f)
    \\[8pt]
    &S^W_X(t,f) = |H_t(f)|^2
    &&\textbf{Wold-Cramer EPS}\\ 
\end{flalign*}

If $X(t)$ is a UMP, then the impulse response is  $h_X(t) = C(t)h_Y(t-n)$,
and  we also have $S^W_X(t,f) = S^E_X(t,f)$ .\\

\textbf{Thomson's time-dependent Spectrum:} (p. 35 of A. pdf)
\begin{flalign*}
    \hat S^W_X(t,f) 
    &= \frac{N}{M}\left| \sum_{m=0}^{M-1}\beta_m(t)\sum_{n=0}^{N-1} X(n)\beta_m(n)e^{-i2\pi fn} \right|^2
    &&\stackrel{\text{(Est. of WC Evol Spectrum)}}{\text{Evolutionary Periodogram}}&
    \\&\\
    \hat S^H_X(t,f) 
    &= \frac{N}{K}\left| \sum_{k=0}^{K-1}\nu_k(t)\sum_{n=0}^{N-1} X(n)\nu_k(n)e^{-i2\pi fn} \right|^2
    &&\stackrel{\text{(Est. of Thomson's t.d. Spectrum)}}{\text{High Resolution Spectrogram}}&\\
\end{flalign*}

\textbf{NOTE:} The evolutionary periodogram actually does use orthogonal tapers $\{\beta_m\}$ (Kayhan, 1994). Swap in slepians and you're at HRS.\\ 

\textbf{NOTE:} HRS might be restrictive in assuming $K$ isn't dependent on $t,f.$
}
% ---------

% ---------
\subsubsection{89 Melard and Schutter - CONTRIBUTIONS TO EVOLUTIONARY SPECTRAL THEORY}
\begin{itemize}
    \item generalization from time invariant coherence to time dependent
    \item improved upper bound for bias due to non-stationarity
\end{itemize}
% ---------

% ---------
\subsubsection{2018 Senay - Time-frequency BSS of biosignals}
\begin{itemize}
    \item applications in medicine
    \item Evolutionary Slepian Transform (EPS with DPSS)
\end{itemize}
% ---------

% ---------
\subsubsection{Azadeh and dAVE}
\begin{itemize}
    \item HRS
    \item SWHRS
    \item MTFSE \& BCMTFSE
    \item check what the ITFSE is
\end{itemize}

{\color{bluu}\textbf{Sliding Window HRS}
Let $X(t)$ be discrete, zero-mean, Gaussian, NON-stationary. 
Let $B<<N$ be the length of a TIME window; Azadeh chooses the overlap to be $B-1$.
Index the blocks by "base-time" (left edge of window) $b\in\{0,\dots,N-B\}$

\begin{flalign*}
    \hat S_{X,b}(t,f) 
    &= \frac{B}{K}\left| \sum_{k=0}^{K-1}\tilde\nu_k(t-b)\sum_{n=0}^{B-1} X_b(n+b)\tilde\nu_k(n)e^{-i2\pi fn} \right|^2
    \\&\\
    t &\in \{b, b+1, \dots, b+B-1\} \\
    \tilde \nu_k(t) &= \nu_k(t)(B,W)
\end{flalign*}

NOTE: there are $B-1$ estimates of $S_x(t,f)$ per $f$, differing by $b$. 
\\

\textbf{\large Time Boundaries}\\[5pt]
\textbf{Using the middle of the blocks}

Define a new time sequence for block centers: $t_b = b + \lceil (B/2)\rceil$
Then for $t$ between $\lceil (B/2)\rceil$ and $N-1-\lceil (B/2)\rceil$, estimate $S_X(t,f)$ by $\hat S_{X,b}(t_b,f)$ at $b = t-\lceil (B/2)\rceil$. Outside that region is the *time boundary region* which is unavailable via this method. 
Equations 3.32 (modified time-frequency spectrum estimator: $\hat S_{X,b,M}(t_b,f)$), 3.33 (time-derivative of the time-frequency spectrum estimator: $\hat S_{X,b,T}(t_b,f)$) will be used to generalize the technique to include time boundaries; they're defined using Nonstationary Quadratic Inverse Theory.\\

\textbf{Estimation at Time Boundaries}\\
IDEA: extrapolate from border of "available" ($t_b$ range) $\hat S_{X,b,M}(t,f)$ by following the slope $\hat S_{X,b,M}(t,f)$ until you hit $t$. (eq. 3.34)
\begin{itemize}
    \item why do we have to use modified rather than basic estimate? Why not use eq 3.27?*
    \item Is the point that it forces the derivative to exist? Or that it lets us find the dervative?
    \item Azadeh footnote (page 40 of PDF): assume derivative of the discrete sample of a continuous-time process = discrete sample of derivative of continuous time process\\
\end{itemize}

\textbf{MTFSE, TDTFSE, BCMTFSE, and ITFSE}

{\small\begin{flalign*}
    &\textbf{SWHRS:}& 
    \hat S_{X,b}(t,f) &= \frac{B}{K}\left|
                                    \sum_{k=0}^{K-1}\stackrel{(B,W)}{\nu_k(t-b)}\;
                                    \sum_{n=0}^{B-1}X_{n+b}^{(b)}\stackrel{(B,W)}{\nu_k(n)}e^{-i2\pi fn}
                                    \right|^2
    \\&\\
    &\textbf{MTFSE:}& 
    \hat S_{X,b,M}(t_b,f) &= \frac{\frac{K}{\alpha_0}\sum_{t=b}^{b+B-1}\hat S_{X,b}(t,f)A_0(t-b)}
                                  {\sum_{t=b}^{b+B-1} A_0(t-b)}
    \\&\\
    &\textbf{TDTFSE:}& 
    \hat S^{(1)}_{X,b,T}(t_b,f) &= \frac{\frac{K}{\alpha_1}\sum_{t=b}^{b+B-1}\hat S_{X,b}(t,f)A_1(t-b)}
                                  {\sum_{t=b}^{b+B-1} A_1(t-b)(t-t_b)}
    \\&\\
    &\textbf{BCMTFSE:}&
    \hat S_{X,b,P}(t_b\pm h,f) &= \hat S_{X,b,M}(t_b,f) \pm h\hat S^{(1)}_{X,b,T}(t_b,f)\\&\\&\\
\intertext{\textbf{Finding The eigenvectors }$A_j(t):$}
    &\stackrel{\text{(To solve: }A_j)}{\text{Eigenvalue Eq:}}&
    \alpha_jA_j(t) &= N\sum_{m=0}^{N-1} \left[\frac{\sin\Big(2\pi W(t-m)\Big)}{\pi(t-m)}\right]A_j(m)
    \\
    &\stackrel{\phantom{\Big(}j\in\{0,1,\dots,\lfloor 4NW\rfloor \}}{\text{Eigenvalues:}}&
    \alpha_j &\approx \max\big( 2NW - j/2, 0 \big) \qquad 
\end{flalign*}}}

% ---------

% ---------------------------------
\subsection{Uniformly modulated processes}

% ---------
\subsubsection{Priestly}
\begin{itemize}
    \item Recall oscillatory
    \item UMPs
\end{itemize}
% ---------

% ---------
\subsubsection{Azadeh}
\begin{itemize}
    \item Detection using log-TFS
    \item Estimating C \\
\end{itemize}

{\color{redd}\textbf{Estimating C}
In matrix/vector notation, $\mathbb S_X = \vec {C^2} \vec S_Y$. 
We use singular value decomposition on the estimated TFS:
\[
\hat {\mathbb S}_X = UDV^T \\\;\\
\text{Dimensions:}\quad
\begin{cases}
    \hat {\mathbb S}_X &\leftarrow N\times N_f \\
    U &\leftarrow N\times N \\
    D &\leftarrow N\times N_f \\
    V &\leftarrow N_f \times N_f
\end{cases}
\]
In Section 4.2, Azadeh explains that the rank of the estimated LTFS matrix $\ln\hat{\mathbb S}_X$ 
gives insight into the structure of $X$ as follows:
\begin{enumerate}
    \item Rank 1 $\implies X$ stationary (Rank 0 for $\mathbb S_X$)
    \item Rank 2 $\implies X$ UMP (Rank 1 for $\mathbb S_X$)\\
\end{enumerate}

\textbf{REASON: }
\[\ln \Big(C(t)^2S_Y(f)\Big) = 2\ln\big(C(t)\big) + \ln\big(S_Y(f)\big)\]
Each term has its own matrix of rank 1 or less. (first: 2-vector times $\ln\big(C(t)\big)$-vector. 
second: 1-vector times $\ln\big(S_Y(f)\big)$-vector). 
If $C(t) = 1$ constantly (stationary) then its log is zero, making the first matrix zero rank.\\
Then the LTFS matrix is either 0+1 or 1+1.\\

Getting the rank of a random matrix is complicated.
Assuming $X(t)$ is UMP, there must only be one non-zero singular value for $\hat {\mathbb S}_X$. So we can simplify:
\[\hat {\mathbb S}_X \approx U_1 d_1 V_1^T\]
Where $U_1, V_1$ are the first columns of $U,V$ and $d_1$ is the first singular value of $\hat{\mathbb S}_X$.\\

Now factor $d_1$ into some pair of nonzero reals $b_u, b_v$. Then
\[\hat {\mathbb S}_X \approx U_1 d_1 V_1^T = (b_uU_1)(b_vV_1)^T.\]
There are infinitely many numbers which qualify for $b_u, b_v$, so this is ambiguous...
We assume $C(t_0) = 1 = C^2(t_0)$ for some $t_0 \in \{0, \dots, N-1\}$. 
Then $S_X(t_0, f) = S_Y(f) \; \forall f$. ($X$ is now a "normal" UMP). \\

Now choose $b_u$ so that $b_uU_1\Big[t_0^{th} \text{ entry}\Big] = 1$. Then $B_v = l_1/b_u$. Now your estimates are:
\[\hat{\vec{C}}^{\,2} \stackrel{\small def}{=} b_uU_1 \qquad\qquad \hat{\vec S}_y \stackrel{\small def}{=} b_vV_1^T\]

\textbf{Least Squares Estimate}}
% ---------


\subsection{Quadratic Inverse Theory}
% ---------
\subsubsection{Thomson 90 - Quadratic-inverse spectrum estimates: applications to palaeoclimatology}
% ---------

% ---------
\subsubsection{Thomson 94 - An overview of multiple-window and quadratic-inverse spectrum estimation methods}
\begin{itemize}
    \item Review of quadratic inverse theory in spectrum estimation for harmonizable processes
    \item T-F decomposition (follows from above)
    \item Introduce idea of a spectrumâ€™s time-derivative - IN USE
\end{itemize}
% ---------

% ---------
\subsubsection{Thomson 2005 - Quadratic-Inverse Expansion of the Rihaczek Distribution}
\begin{itemize}
    \item Skye how does this build on the 94 paper? is it useful?
\end{itemize}
% ---------

















\end{document}
