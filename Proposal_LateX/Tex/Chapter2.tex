\chapter{Background and Literature Review} \label{chap:chap2}
\setlength{\parindent}{0pt}

%% --- SECTION: EPS ------------ %%
\section{Evolutionary Power Spectra}
\colorbox{briest}{\textbf{Section Introduction: } 
       \text{Motivation and Desirable Spectral Properties}}\\
\color{priest}
\textbf{1} $\quad$ We want interpretable, easily estimable spectra for non-stationary series. \\ 
\textbf{2} $\quad$ These should reveal stochastic structure, and support linear prediction/filtering.\\
\textbf{3} $\quad$ Evolutionary Spectra reveal frequency structure near some particular time point.\\
\textbf{4} $\quad$ Some non-stationary series can be broken into multiple stationary sub-series \\
\textbf{5} $\quad$ These sub-series may have distinct autocovariance functions \\

\color{black}
\colorbox{briest}{\textbf{Example: Short Time Fourier Transforms}}\\
\color{priest}
Obtain average spectrum in a neighbourhood about time instant $t$.
\[ x_t = x_t^{(i)} \qquad\qquad t_i < t \leq t_{i+1} \]

$\star\quad$ Each interval has a corresponding spectrum. \\
$\star\quad$ Interpreted similarly to classical (as for stationary series) spectra \\
$\star\quad$ Describes power-frequency distribution *locally* about an *instant* in time

%% --- SUBSECTION: Oscillatory Processes and UMPs ------------ %%
{\color{black}\subsection{Oscillatory and Uniformly Modulated Processes}}
\color{azadeh}\colorbox{bzadeh}{\color{black}\textbf{Oscillatory Functions/Processes:}}  (p. 20 of A. pdf) \\
Let $X(t)$ be discrete, complex, zero-mean, finite variance. Suppose we can write the covariance function of $X$ as: 
\[
\begin{aligned}
    R_X(t,s) &= \int_{-1/2}^{1/2} \phi_t(f)\phi_s^*(f) S_X(f)df \qquad (3.1) 
    \\[5pt]
    \phi_\cdot &: [-1/2, 1/2] \to \mathbb{C} \\
    S_X &: [-1/2, 1/2] \to \mathbb{R}
\end{aligned}
\]

Fix $f_0\in [-1/2, 1/2]$. Suppose $\phi_t(f_0)$ has an FT which maxes out at the frequency $\theta(f_0) \in [-1/2, 1/2]$. Then we can write 
\begin{align*}
    \phi_t(f) &= M_t(f)e^{i2\pi\theta(f)t} \\
         X(t) &= \int_{\mathbb{R}} M(t,f)\, e^{i2\pi ft}\, dZ(f) \qquad \forall t\in\mathbb{R}
\end{align*}
Where $M_t(f_0)$ has an FT which maxes out at zero frequency. \textit{("Slowly varying").} Then $\phi_t$ is an oscillatory func., and $X$ is an oscillatory process \\

\colorbox{bzadeh}{\color{black}\textbf{Uniformly Modulated Processes:}}  (p. 30 of A. pdf)\\
Let $X(t)$ be an oscillatory process and $\{C(t)\}_{t\in\mathbb Z}$ be non-negative, "slowly varying". Let $Y(t)$ be zero-mean, stationary. Then X is a uniformly modulated process with evolutionary spectrum $S^E_X(t,f) = C^2(t)S_Y(f)$ \\

\color{black} \subsection{An Alternative notion of Frequency} \color{priest}
Stationary $x_t$ have spectral representation $x_t = \int_{-\infty}^\infty e^{it\omega}dZ(\omega)\qquad (11.1.3)$

By (11.1.3), any stationary process can be represented as the sum of \textit{sines/cosines at varying frequencies} with \textit{random amplitudes/phases.} This lets us discuss the power (variance) contribution of a component distinguished by frequency $\omega$. 

Suppose $x_t$ is a continuous, non-stationary process with ACVF $\gamma(s,t)$, and consider a function $\rho(\omega,t)$ such that \[ E\Big[x^2_t\Big] = \int_{\mathbb{R}} \rho(\omega,t) d\omega \qquad (11.1.1) \] The issue is: $\rho$ is not necessarily a physically interpretable time dependent function of frequency.\\

\color{black} \colorbox{briest}{\textbf{Example: Naive Approach to Time Dependent Spectra}} \color{priest}
\[\small \begin{aligned}
    &{\footnotesize\text{"Local" ACVF }} 
    &\gamma_\tau(t) &\stackrel{def}{=} \gamma\left(t-\frac{\tau}{2},\, t+\frac{\tau}{2}\right)
    & \;
    \\&&&&\\
    &{\footnotesize\text{Naive time dependent SDF}}
    &\psi_t(\omega) &= \frac{1}{2\pi}\int_{-\infty}^\infty \gamma\left(t-\frac{\tau}{2},\, t-\frac{\tau}{2}\right) e^{-i\omega\tau}d\tau 
    & \quad(11.1.2 a)    
    \\&&&&\\
    &{\footnotesize\begin{aligned}
        &\colorbox{bzadeh}{\color{black}\text{Azadeh's Notation}}\\
        &\color{azadeh}\text{(Mean of Wigner-Ville)}
    \end{aligned}}
    & \color{azadeh} W_X(t,f) 
    & \color{azadeh} = \int_{\mathbb{R}} 
    E\left[
    X\Big(t + \frac{\tau}{2}\Big)
    \overline{X\Big(t - \frac{\tau}{2}}\Big)
    \right]
    e^{-i2\pi f\tau} d\tau
    & \color{azadeh} \forall t,f\in \mathbb{R}
    \\&&&&\\
    &{\footnotesize\text{FT pair at } t \text{ (} \tau = 0)} 
    &\gamma(t,t) &= \int_{-\infty}^{\infty} \psi_t(\omega)d\omega
    & \quad(11.1.2 b)
\end{aligned}\]

$\star\quad \psi_t(\omega)$ may take \textit{negative values.} So there isn't a good physical interpretation \\
$\star\quad \psi_t(\omega)$ not obviously interpretable wrt structure of $x_t$ \\
$\star\quad$ We need to define frequency without using sinusoids.\\

\colorbox{briest}{\color{black}\textbf{Example: "Frequency" in the context of evolving spectra}}\\
Suppose $x_t$ is a (deterministic) dampened sine:
\[ x_t = Ae^{-t^2/\alpha^2}\cos (\omega_0 t + \phi) \qquad (11.1.4) \]

$\star\quad$ The FT of $x_t$ is 2 Gaussian Functions centred at $\omega_0, -\omega_0$, respectively.\\
$\phantom{\star\quad}$ The width of the Gaussian functions is inversely proportional to $\alpha$. \\
$\star\quad$ $x_t$ as a sum of sinusoids must include all frequency components 

\hangindent=20pt
$\star\quad$ $x_t$ can be represented by only the two frequencies $\omega_0$ and $-\omega_0$. Each component then has time varying amplitude $Ae^{-t^2/\alpha^2}$. If the time interval of observation is small relative to $\alpha$, $x_t$ will resemble (eq. 11.1.4) \\ 

\colorbox{briest}{\color{black}\textbf{Non-oscillatory processes: }} \\ 
\color{priest} Suppose $y_t$ behaves locally similar to a sine wave with fixed frequency and smooth amplitude modulation.

$\star\quad$ Frequency domain of \textit{oscillatory process:} FT is concentrated about $\omega_0$\\
$\star\quad$ What if $y_t = \ln(\omega t)$? Then frequency has no meaning.\\
$\star\quad$ Consider instead: the Fourier transform's absolute maximum. (Say, at point $\omega_1$)\\

\colorbox{briest}{\color{black}\textbf{Summary}}

\hangindent = 20pt
\textbf{1} $\;$ Autocovariance can be taken for balls surrounding a time $t$ of interest, and the Fourier transform of this ACVF has a pair converging to the variance of $x_t$ as the radius shrinks. However, this expression can take negative values and is often therefore not meaningful.

\hangindent = 20pt
\textbf{2} $\;$ Time series can be represented by\\
\textbf{a)} infinitely many frequency components, with constant amplitudes OR\\
\textbf{b)} frequency components with time-varying amplitudes.

\hangindent = 20pt
\textbf{3} $\;$ If $x_t$ does not oscillate, we define its "frequency" to be the absolute maximum of its freq. domain representation. We interpret this as the freq for a local sinusoidal representation whose amplitude is being modulated.

%% --- SECTION: Spectrograms ------------ %%
\color{azadeh}\section{\color{black} Spectrograms and Uniformly Modulated Processes}

\colorbox{bzadeh}{\color{black}\textbf{Wold-Cramer Evolutionary Spectra}}  (p. 30 of A. pdf)\\ 
Let $X(t)$ be discrete, real, zero-mean, NON-stationary. Let $\epsilon(t)$ be white noise (0,1). Then $X$ is the output of a causal $(n\leq t)$ time-varying linear filter.
\[
\begin{aligned}
    X(t) &= \sum_{n=-\infty}^t h(t,n)\epsilon(n)                &&\text{Linear Filter}
    \\[5pt]
    \epsilon(t) &= \int_{-1/2}^{1/2}e^{i2\pi ft}dZ_\epsilon (f) &&\text{Spec rep }(\epsilon \text{ stationary})
    \\[5pt]
    H_t(f) &= \sum_{n=-\infty}^t h(t,n) e^{-i2\pi f(t-n)}       &&\text{Generalized TF}
    \\[5pt]\hline\\[-15pt]
    \implies X(t) &= \int_{-1/2}^{1/2} H_t(f) e^{i2\pi ft}dZ_\epsilon (f)
    \\[5pt]
    S^W_X(t,f) &= |H_t(f)|^2
    &&\textbf{Wold-Cramer EPS} 
\end{aligned}
\]\\
If $X(t)$ is UMP, the impulse response is  $h_X(t) = C(t)h_Y(t-n)$ and $S^W_X(t,f) = S^E_X(t,f)$ \\

\colorbox{bzadeh}{\color{black}\textbf{Thomson's time-dependent Spectrum}} \\

\colorbox{bzadeh}{\color{black}\textbf{Evolutionary Periodogram vs. High Resolution Spectrogram}}
\\
\[
\begin{aligned}
    &  \stackrel{\text{(Estimate of Wold-Cramer Evolutionary Spectrum)}}{\text{Evolutionary Periodogram}} 
    \\
       \hat S^W_X(t,f) 
    &= \frac{N}{M}\left| \sum_{m=0}^{M-1}\beta_m(t)\sum_{n=0}^{N-1} X(n)\beta_m(n)e^{-i2\pi fn} \right|^2
    \\&\\
    &  \stackrel{\text{(Estimate of Thomson's time-dependent Spectrum)}}{\text{High Resolution Spectrogram}} 
    \\
       \hat S^H_X(t,f) 
    &= \frac{N}{K}\left| \sum_{k=0}^{K-1}\nu_k(t)\sum_{n=0}^{N-1} X(n)\nu_k(n)e^{-i2\pi fn} \right|^2 
\end{aligned}
\]

\hangindent = 50pt
\textbf{NOTE:} The evolutionary periodogram actually does use orthogonal tapers $\{\beta_m\}$ (Kayhan, 1994). Swap in slepians and you're at HRS.

\textbf{NOTE:} HRS might be restrictive in assuming $K$ isn't dependent on $t,f.$\\

\colorbox{bzadeh}{\color{black}\textbf{Sliding Window HRS}}
Let $X(t)$ be discrete, zero-mean, Gaussian, NON-stationary. Let $B<<N$ be the length of a TIME window; Azadeh chooses the overlap to be $B-1$. Index the blocks by "base-time" (left edge of window) $b\in\{0,\dots,N-B\}$
\[
\begin{aligned}
    \hat S_{X,b}(t,f) 
    &= \frac{B}{K}\left| \sum_{k=0}^{K-1}\tilde\nu_k(t-b)
                         \sum_{n=0}^{B-1} X_b(n+b)\tilde\nu_k(n)e^{-i2\pi fn} \right|^2
    \\&\\
    t &\in \{b, b+1, \dots, b+B-1\} \\
    \tilde \nu_k(t) &= \nu_k(t)(B,W)
\end{aligned}
\]

\textbf{NOTE:} there are $B-1$ estimates of $S_x(t,f)$ per $f$, differing by $b$. \\

\textbf{\large Using the middle of the blocks} \\
Define a new time sequence for block centers: $t_b = b + \lceil (B/2)\rceil$
Then for $t$ between $\lceil (B/2)\rceil$ and $N-1-\lceil (B/2)\rceil$, estimate $S_X(t,f)$ by $\hat S_{X,b}(t_b,f)$ at $b = t-\lceil (B/2)\rceil$. Outside that region is the *time boundary region* which is unavailable via this method. Equations 3.32 (modified time-frequency spectrum estimator: $\hat S_{X,b,M}(t_b,f)$), and 3.33 (time-derivative of the time-frequency spectrum estimator: $\hat S_{X,b,T}(t_b,f)$) will be used to generalize the technique to include time boundaries; they're defined using Nonstationary Quadratic Inverse Theory.\\

\textbf{\large Estimation at Time Boundaries} \\
IDEA: extrapolate from border of "available" ($t_b$ range) $\hat S_{X,b,M}(t,f)$ by following the slope $\hat S_{X,b,M}(t,f)$ until you hit $t$. (eq. 3.34) \textit{Azadeh footnote (page 40 of PDF): assume derivative of the discrete sample of a continuous-time process = discrete sample of derivative of continuous time process.}\\
\colorbox{bzadeh}{\color{black}\textbf{HRS, SWHRS, MTFSE, TDTFSE, and BCMTFSE}}

\[\boxed{
\begin{aligned} &\\[-20pt]
    \quad&\textbf{HRS:}&
    \hat S(t,f)               &= \left|\frac{N}{K}\sum_{k=0}^{K-1}\sqrt{\lambda_k}\nu_k(t)
                                 \sum_{n=0}^{N-1}\nu_k(n)X(n)e^{-i2\pi fn}\right|^2
    \\[10pt]                                          
    &\textbf{SWHRS:}& 
    \hat S_{b}(t^*,f)         &= \frac{B}{K}\left| \sum_{k=0}^{K-1}\tilde\nu_k(t^*-b)
                                 \sum_{n=1}^{B} X_b(n+b)\tilde\nu_k(n)e^{-i2\pi fn} \right|^2\quad
    \\[10pt]
    &\textbf{MTFSE:}& 
    \hat S_{b,M}(t_b,f)       &= \frac{\frac{K}{\alpha_0}\sum_{t=b}^{b+B-1}\hat S_{b}(t,f)A_0(t-b)}
                                 {\sum_{t=b}^{b+B-1} A_0(t-b)}
    \\[10pt]
    &\textbf{TDTFSE:}& 
    \hat S^{(1)}_{b,T}(t_b,f) &= \frac{\frac{K}{\alpha_1}\sum_{t=b}^{b+B-1}\hat S_{b}(t,f)A_1(t-b)}
                                 {\sum_{t=b}^{b+B-1} A_1(t-b)(t-t_b)}
    \\[10pt]
    &\textbf{BCMTFSE:}&
    \hat S_{b,P}(t_b\pm h,f)  &= \hat S_{b,M}(t_b,f) \pm h\hat S^{(1)}_{b,T}(t_b,f)
    \\[7pt]
    \hline \\[-25pt]
    && t^* &\in \{b, b+1, b+B-1\}
    \\
    && b   &\in \{0,1,\dots, N-B\}
    \\
    && t_b &= b + \lceil B/2\rceil
    \\[7pt]
    \hline \\[-15pt]
    &\stackrel{\text{(To solve: }A_j)}{\text{Eigenvalue Eq:}}&
    \alpha_jA_j(t) &= N\sum_{m=0}^{N-1} \left[\frac{\sin\Big(2\pi W(t-m)\Big)}{\pi(t-m)}\right]^2A_j(m)
    \\[5pt]
    &\stackrel{j\in\{0,1,\dots,\lfloor 4NW\rfloor \}}{\text{Eigenvalues:}}&
    \alpha_j &\approx \max\big( 2BW - j/2, 0 \big) \qquad
\end{aligned}}
\]

\colorbox{bzadeh}{\color{black}\textbf{Estimating C}} \\
In matrix/vector notation, $\mathbb S_X = \vec {C^2} \vec S_Y$. 
We use singular value decomposition on the estimated TFS:
\[
\hat {\mathbb S}_X = UDV^T \qquad 
\text{Dimensions:}\quad
{\small\begin{cases}
    \hat {\mathbb S}_X &\leftarrow N\times N_f \\
    U &\leftarrow N\times N \\
    D &\leftarrow N\times N_f \\
    V &\leftarrow N_f \times N_f
\end{cases}}
\]

In Section 4.2, Azadeh explains that the rank of the estimated LTFS matrix $\ln\hat{\mathbb S}_X$ gives insight into the structure of $X$ as follows: \\
\hspace*{100pt} Rank 1 $\implies X$ stationary (Rank 0 for $\mathbb S_X$) \\
\hspace*{100pt} Rank 2 $\implies X$ UMP (Rank 1 for $\mathbb S_X$) 
\[ \textbf{REASON: } \ln \Big(C(t)^2S_Y(f)\Big) = 2\ln\big(C(t)\big) + \ln\big(S_Y(f)\big) \]

Each term has its own matrix of rank 1 or less. (first: 2-vector times $\ln\big(C(t)\big)$-vector. second: 1-vector times $\ln\big(S_Y(f)\big)$-vector). If $C(t) = 1$ constantly (stationary) then its log is zero, making the first matrix zero rank. Then the LTFS matrix is either 0+1 or 1+1. 

Getting the rank of a random matrix is complicated. Assuming $X(t)$ is UMP, there must only be one non-zero singular value for $\hat {\mathbb S}_X$. So we can simplify:
\[ \hat {\mathbb S}_X \approx U_1 d_1 V_1^T \]
Where $U_1, V_1$ are the first columns of $U,V$ and $d_1$ is the first singular value of $\hat{\mathbb S}_X$. Now factor $d_1$ into some pair of nonzero reals $b_u, b_v$. Then \[ \hat {\mathbb S}_X \approx U_1 d_1 V_1^T = (b_uU_1)(b_vV_1)^T. \] There are infinitely many numbers which qualify for $b_u, b_v$, so this is ambiguous... We assume $C(t_0) = 1 = C^2(t_0)$ for some $t_0 \in \{0, \dots, N-1\}$. Then $S_X(t_0, f) = S_Y(f) \; \forall f$. ($X$ is now a "normal" UMP). Now choose $b_u$ so that $b_uU_1\Big[t_0^{th} \text{ entry}\Big] = 1$. Then $B_v = l_1/b_u$. Now your estimates are:
$$
\hat{\vec{C}}^{\,2} \stackrel{\small def}{=} b_uU_1 \qquad\qquad \hat{\vec S}_y \stackrel{\small def}{=} b_vV_1^T
$$
\color{black}


